{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Deep Q Network for finite action space\n",
    "TODO: \n",
    "- DQN using Double Q learning with target network, replay buffer, target network, and epsilon greedy policy"
   ],
   "id": "a1ea5581e0df1a37"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T16:47:25.283563Z",
     "start_time": "2024-11-02T16:47:23.546087Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import homework2 as hw2\n",
    "from homework2 import Hw2Env\n",
    "\n",
    "import time\n",
    "import numpy as np"
   ],
   "id": "2462fa8eb799aa87",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define the network\n",
    "##### it will take the high level state of the simulation (ee_pos, obj_pos, goal_post) as input and give the Q values for each action as output "
   ],
   "id": "8268de11d944df7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T16:47:27.425720Z",
     "start_time": "2024-11-02T16:47:27.421086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 8)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ],
   "id": "e80e3e0c4f30592a",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Define the agent\n",
    "##### it will have the Q network and the target network, and will use the epsilon greedy policy and the replay buffer methods. \n",
    "- lr: learning rate, used in the optimizer\n",
    "- gamma: discount factor that describes how much the agent values future rewards\n",
    "- epsilon: the probability of selecting a random action instead of the greedy optimal one\n",
    "- tau: used for polyak averaging, the rate at which the target network is updated"
   ],
   "id": "f8edbff39c84cef3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T16:50:33.740173Z",
     "start_time": "2024-11-02T16:50:33.645888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, lr=0.001, gamma=0.99, epsilon=0.1, tau=0.999):\n",
    "        self.q_network = QNetwork(state_dim)\n",
    "        self.target_network = QNetwork(state_dim)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.tau = tau\n",
    "        self.replay_buffer = []\n",
    "        \n",
    "    # epsilon greedy policy to select the action\n",
    "    def get_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(0, 8)\n",
    "        else:\n",
    "            return self.get_optimal_action(state)\n",
    "        \n",
    "    def get_optimal_action(self, state):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        return torch.argmax(self.q_network(state)).item()\n",
    "    \n",
    "    # Since we will use Double Q learning, we want to use different networks to compute the reward and the target\n",
    "    # We will use the target network for the target estimation, and as input to the target network we will give the argmax action of the current network, decoupling the action selection (now done by the current network) from the target estimation (done by the target network)\n",
    "    def estimate_target(self, reward, next_state):\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        print(np.shape(next_state))\n",
    "        action = torch.argmax(self.q_network(next_state)).item()\n",
    "        y = reward + (self.gamma * self.target_network(next_state)[action].item())\n",
    "        return y\n",
    "    \n",
    "    def update_q_network(self, state, action, target):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        target = torch.tensor(target, dtype=torch.float32)\n",
    "        loss = nn.MSELoss()(self.q_network(state)[action], target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def load_target_network_with_current_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        \n",
    "    def update_target_network_using_polyak_avg(self):\n",
    "        for target_param, param in zip(self.target_network.parameters(), self.q_network.parameters()):\n",
    "            target_param.data.copy_(self.tau*param.data + (1-self.tau)*target_param.data)\n",
    "            \n",
    "    def add_to_replay_buffer(self, transition):\n",
    "        if len(self.replay_buffer) > 100000:\n",
    "            self.replay_buffer.pop(0)\n",
    "        self.replay_buffer.append(transition)\n",
    "        \n",
    "    def sample_replay_buffer(self, batch_size):\n",
    "        if len(self.replay_buffer) < batch_size:\n",
    "            return self.replay_buffer\n",
    "        return np.random.choice(self.replay_buffer, batch_size)\n",
    "    \n",
    "    def train(self, batch_size):\n",
    "        transitions = self.sample_replay_buffer(batch_size)\n",
    "        for transition in transitions:\n",
    "            state, action, reward, next_state = transition\n",
    "            target = self.estimate_target(reward, next_state)\n",
    "            self.update_q_network(state, action, target)\n",
    "        self.update_target_network_using_polyak_avg()\n",
    "    \n",
    "    \n",
    "    def save(self, path):\n",
    "        torch.save(self.q_network.state_dict(), path)\n",
    "        "
   ],
   "id": "7c5e61b91fcdb56a",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Train the agent in the simulation environment",
   "id": "cd17b7fa5fbab2c7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "N_ACTIONS = 8\n",
    "env = Hw2Env(n_actions=N_ACTIONS, render_mode=\"gui\")\n",
    "\n",
    "state_dim = 6\n",
    "agent = DQNAgent(state_dim)\n",
    "\n",
    "n_episodes = 100\n",
    "batch_size = 32\n",
    "for episode in range(n_episodes):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    cum_reward = 0.0\n",
    "    start = time.time()\n",
    "    while not done:\n",
    "        state = env.high_level_state()\n",
    "        print(np.shape(state))\n",
    "        action = agent.get_action(state)\n",
    "        next_state, reward, is_terminal, is_truncated = env.step(action)\n",
    "        agent.add_to_replay_buffer((state, action, reward, next_state))\n",
    "        agent.train(batch_size)\n",
    "        done = is_terminal or is_truncated\n",
    "        cum_reward += reward\n",
    "    end = time.time()\n",
    "    print(f\"Episode={episode}, reward={cum_reward}, RF={env.data.time/(end-start):.2f}\")"
   ],
   "id": "142ed143cd136ea6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7930a6e7286e56b3"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
